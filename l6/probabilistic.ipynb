{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7804892",
   "metadata": {},
   "source": [
    "# Probabilistic retrieval model\n",
    "Compare TfIdf vector space model with Okapi bm25 adding terms enrichment to queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215ebb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1099747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from twitter import TwitterDataset, ENTITY, DOMAIN\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4344557",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pymongo.MongoClient()['twitter']['tweets']\n",
    "tdata = TwitterDataset(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw = set(stopwords.words('english'))\n",
    "tokenizer = TweetTokenizer()\n",
    "tokenize = lambda text: [word for word in tokenizer.tokenize(text.lower()) \n",
    "                         if word not in punctuation and word not in stopw and not word.startswith('http')] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24324165",
   "metadata": {},
   "source": [
    "## Specificity score\n",
    "$$\n",
    "\\delta(w) = p(w) \\log \\frac{p(w)}{q(w)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de031e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(db.find())\n",
    "G = defaultdict(lambda: 0)\n",
    "Q = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for tweet in tqdm(tweets):\n",
    "    try:\n",
    "        text = tweet['text']\n",
    "        annotations = tweet['context_annotations']\n",
    "        tokens = tokenize(text)\n",
    "        for token in tokens:\n",
    "            G[token] += 1\n",
    "            for annotation in annotations:\n",
    "                domain_name = annotation['domain']['name']\n",
    "                Q[domain_name][token] += 1\n",
    "                entity_name = annotation['entity']['name']\n",
    "                Q[entity_name][token] += 1\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfc3d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_query(query, top_k=5):\n",
    "    Qtot = sum(Q[query].values())\n",
    "    Gtot = sum(G.values())\n",
    "    terms = {}\n",
    "    for word, count in Q[query].items():\n",
    "        p_w = count / Qtot\n",
    "        q_w = G[word] / Gtot\n",
    "        terms[word] = p_w * np.log(p_w / q_w)\n",
    "    return [x for x, y in sorted(terms.items(), key=lambda x: -x[1])][:top_k]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d74a1f",
   "metadata": {},
   "source": [
    "## Binary Independence Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8a81bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(db.find())\n",
    "R = defaultdict(lambda: 1)\n",
    "N = defaultdict(lambda: 1)\n",
    "\n",
    "query = 'COVID-19'\n",
    "NumR, NumN = 0, 0\n",
    "\n",
    "for tweet in tqdm(tweets):\n",
    "    try:\n",
    "        text = tweet['text']\n",
    "        tweet_id = tweet['id']\n",
    "        annotations = tweet['context_annotations']\n",
    "        relevant = False\n",
    "        for annotation in annotations:\n",
    "            domain_name = annotation['domain']['name']\n",
    "            entity_name = annotation['entity']['name']\n",
    "            if query == domain_name or query == entity_name:\n",
    "                relevant = True\n",
    "                break\n",
    "        NumN += 1\n",
    "        NumR += 1\n",
    "        if relevant:\n",
    "            NumR += 1\n",
    "        else:\n",
    "            NumN += 1\n",
    "        tokens = set(tokenize(text))\n",
    "        for token in tokens:\n",
    "            if relevant:\n",
    "                R[token] += 1\n",
    "            else:\n",
    "                N[token] += 1\n",
    "    except KeyError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba7cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = lambda word: R[word] / NumR\n",
    "q = lambda word: N[word] / NumN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4deb7a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bmi(query_text, doc_text):\n",
    "    q_words = set(tokenize(query_text)).intersection(set(tokenize(doc_text)))\n",
    "    sum_log = 0\n",
    "    for q_w in q_words:\n",
    "        p_score, q_score = p(q_w), q(q_w)\n",
    "        score = np.log((p_score * (1 - q_score)) / (q_score * (1 - p_score)))\n",
    "        sum_log += score\n",
    "    return sum_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c2b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = 'vaccine and pandemic'\n",
    "ranking = {}\n",
    "for tweet_id, tweet_text in tdata.search_base:\n",
    "    ranking[tweet_id] = bmi(query_text, tweet_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed464815",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = sorted(ranking.items(), key=lambda x: -x[1])[:2]\n",
    "sb = dict(tdata.search_base)\n",
    "for k, score in answers:\n",
    "    print(sb[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df955da1",
   "metadata": {},
   "source": [
    "## The Okapi bm25 system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ebfcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f4025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_corpus = [tokenize(text) for i, text in tdata.search_base]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc87c351",
   "metadata": {},
   "source": [
    "## The TfIdf IR system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eed332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from search import TfIdfSearchEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11940ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = TfIdfSearchEngine(tdata.search_base, tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86e4dd",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc78ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_environment(query, query_type=ENTITY, top_k=5):\n",
    "    ground_truth = tdata.ground_truth(query, query_type=query_type)\n",
    "    extension = extend_query(query, top_k=top_k)\n",
    "    return extension, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab98ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_queries = ['Business & finance', 'COVID-19', 'Boris Johnson', 'Food']\n",
    "domain_queries = ['Politician', 'TV Shows', 'Athlete', 'Sports Event']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8665c17d",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = entity_queries[-1]\n",
    "query_type = ENTITY\n",
    "extension, ground_truth = get_query_environment(query, query_type=query_type, top_k=10)\n",
    "y_true = [1 if i in ground_truth else 0 for i, _ in tdata.search_base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82bf1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extension, len(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8526a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_base = E.search(query)\n",
    "tfidf_ext = E.search(\"{} {}\".format(query, \" \".join(extension)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c484e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_base_scores = np.zeros(len(tdata.search_base))\n",
    "for i, ti, score in tfidf_base:\n",
    "    tfidf_base_scores[i] = score\n",
    "tfidf_ext_scores = np.zeros(len(tdata.search_base))\n",
    "for i, ti, score in tfidf_ext:\n",
    "    tfidf_ext_scores[i] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d99a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "okapi_base_scores = bm25.get_scores(tokenize(query))\n",
    "okapi_ext_scores = bm25.get_scores(tokenize(\"{} {}\".format(query, \" \".join(extension))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39672968",
   "metadata": {},
   "source": [
    "## Precision and recall curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7561166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec18fbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a35085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots(y_true, y_scores):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    rec = list(sorted(recall, key=lambda x: x))\n",
    "    prec = list(sorted(precision, key=lambda x: -x))\n",
    "    iprec = [max(prec[i:]) for i in range(len(prec))]\n",
    "    return rec, iprec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2140e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\n",
    "    ('TfIdf Base', tfidf_base_scores),\n",
    "    ('TfIdf Ext', tfidf_ext_scores),\n",
    "    ('Okapi Base', okapi_base_scores),\n",
    "    ('Okapi Ext', okapi_ext_scores)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a545b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for elabel, escores in experiments:\n",
    "    x, y = plots(y_true, escores)\n",
    "    ax.plot(x, y, label=elabel)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74637440",
   "metadata": {},
   "source": [
    "## Maximum Likelihood for bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d86842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e03f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "U = defaultdict(lambda: 0)\n",
    "B = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "for tweet_id, text in tdata.search_base:\n",
    "    tokens = ['#START'] + tokenize(text) + ['#END']\n",
    "    for token in tokens:\n",
    "        U[token] += 1\n",
    "    for a, b in nltk.ngrams(tokens, n=2):\n",
    "        B[a][b] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "B['joe']['biden'] / U['joe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e71fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    "B['biden']['#END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c382e7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = {}\n",
    "for second_word, freq in B['biden'].items():\n",
    "    probs[second_word] = freq / U['biden']\n",
    "print(sorted(probs.items(), key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a08812",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
